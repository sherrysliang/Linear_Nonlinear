---
title: "US Treasury YTM Analysis"
author: "Shuang (Sherry) Liang"
date: "3/10/2018"
output: html_document
---

## Step 1

```{r}
datapath<-"..."
dat<-
  read.csv(file=paste(datapath,"regressiondata.csv",sep="/"),
           row.names=1,header=TRUE,sep=",")
dim(dat)
head(dat)
```

```{r}
matplot(dat[,-c(8,9,10)],type='l')
```

```{r}
matplot(dat[,-c(9,10)],type='l')
```

#### Brief remark
>  With ups and downs, the US Treasury yields to maturity (YTM) experience a downward trend in general. While the change of output is in consistency with the change of the US Treasury YTM, the output is higher than the US Treasury YTM before the point aronud 1300, and lower than YTM afterwards with the gap increasing.

## Step 2

```{r}
Input1.linear.Model<-lm(Output1~USGG3M,data=dat)
```

```{r}
c(Total.Variance=var(dat[,8]),Unexplained.Variance=summary(Input1.linear.Model)$sigma^2)
# Total Variance = sum((dat$Output1-mean(dat$Output1))^2)/8299 -------------  (n-1)
# Unexplained Variance = sum((Input1.linear.Model$fitted.values-dat$Output1)^2)/8298 --  (n-2)---residuals, df.residuals
#  Explained Variance =· sum((Input1.linear.Model$fitted.values-mean(dat$Output1))^2)/8299
```

```{r}
(names(Input1.linear.Model))
(names(summary(Input1.linear.Model)))
```

```{r}
summary(Input1.linear.Model)
```


```{r}
matplot(dat[,8],type="l",xaxt="n")
lines(Input1.linear.Model$fitted.values,col="red")
```

```{r}
Input2.linear.Model<-lm(Output1~USGG6M,data=dat)
summary(Input2.linear.Model)
```

```{r}
matplot(dat[,8],type="l",xaxt="n")
lines(Input2.linear.Model$fitted.values,col="red")
```

```{r}
Input3.linear.Model<-lm(Output1~USGG2YR,data=dat)
summary(Input3.linear.Model)
```

```{r}
matplot(dat[,8],type="l",xaxt="n")
lines(Input3.linear.Model$fitted.values,col="red")
```

```{r}
Input4.linear.Model<-lm(Output1~USGG3YR,data=dat)
summary(Input4.linear.Model)
```

```{r}
matplot(dat[,8],type="l",xaxt="n")
lines(Input4.linear.Model$fitted.values,col="red")
```

```{r}
Input5.linear.Model<-lm(Output1~USGG5YR,data=dat)
summary(Input5.linear.Model)
```

```{r}
matplot(dat[,8],type="l",xaxt="n")
lines(Input5.linear.Model$fitted.values,col="red")
```

```{r}
Input6.linear.Model<-lm(Output1~USGG10YR,data=dat)
summary(Input6.linear.Model)
```

```{r}
matplot(dat[,8],type="l",xaxt="n")
lines(Input6.linear.Model$fitted.values,col="red")
```

```{r}
Input7.linear.Model<-lm(Output1~USGG30YR,data=dat)
summary(Input7.linear.Model)
```

```{r}
matplot(dat[,8],type="l",xaxt="n")
lines(Input7.linear.Model$fitted.values,col="red")
```

#### Plot the output variable together with the fitted values and Analyze
>  Most of the model plots appear to fit fairly well. The forth model using three year bond rate as predictor appears to fit the data most accurately, with the adjusted R square of 0.9979 being the highest. As the YTM get shorter or longer from the 3 year term, the unexplained variance in the data rises as a percentage of the total variance.

```{r}
# Collect all slopes and intercepts in one table and print this table. Try to do it in one line using apply() function.
(Slope.Intercept.Table<-t(sapply(1:7, function(x) lm(dat[,8]~dat[,x])$coefficient)))
```

## Step 3

```{r}
#Fit linear regression models using single output (column 8 Output1) as input and each of the original inputs as outputs.
#Collect all slopes and intercepts in one table and print this table.
(Slope.Intercept.Table2<-t(sapply(1:7, function(x) lm(dat[,x]~dat[,8])$coefficient)))
```

```{r}
(r2.list<-Slope.Intercept.Table[,2]*Slope.Intercept.Table2[,2])
```

#### any differences between these seven models? 
>  Since β1=r*(Sy/Sx) (y is output) and β1'=r*(Sx/Sy) (x is output), we can get r2 by multiply β1 and β1'. As we can see the output in r2.list, the results are the same with R2 for each model we have built. The r2 of the fourth model is the largest -- 0.9979215, confirming our previous conclusion from the plots-- the predicted line is the closest with the observed line.

## Step 4

Estimate logistic regression using all inputs and the data on FED tightening and easing cycles.
```{r}
datLogistic<-data.matrix(dat,rownames.force="automatic")
# head(datLogistic)
```

Prepare the easing-tightening data.
Make the easing column equal to 0 during the easing periods and NA otherwise.
Make the tightening column equal to 1 during the tightening periods and NA otherwise.
```{r, echo=FALSE}
# Create columns of easing periods (as 0s) and tightening periods (as 1s)
EasingPeriods<-datLogistic[,9]
EasingPeriods[datLogistic[,9]==1]<-0
TighteningPeriods<-datLogistic[,10]
# Check easing and tightening periods
cbind(EasingPeriods,TighteningPeriods)[c(550:560,900:910,970:980),]
```

Remove the periods of neither easing nor tightening.
```{r}
All.NAs<-is.na(EasingPeriods)&is.na(TighteningPeriods)
datLogistic.EasingTighteningOnly<-datLogistic
datLogistic.EasingTighteningOnly[,9]<-EasingPeriods
datLogistic.EasingTighteningOnly<-datLogistic.EasingTighteningOnly[!All.NAs,]
datLogistic.EasingTighteningOnly[is.na(datLogistic.EasingTighteningOnly[,10]),10]<-0
# Binary output for logistic regression is now in column 10
```

Plot the data and the binary output variable representing easing (0) and tightening (1) periods.
```{r}
matplot(datLogistic.EasingTighteningOnly[,-c(9,10)],type="l",ylab="Data and Binary Fed Mode")
lines(datLogistic.EasingTighteningOnly[,10]*20,col="red")
```

Fit logistic model with 3M predictor. Interpret the summary/coefficients, and display the plot which overlays the predicted values onto the above plot. Say anything that occurs to you.
```{r}
# Estimate logistic regression with 3M yields as predictors for easing/tightening output.
LogisticModel.TighteningEasing_3M<-glm(datLogistic.EasingTighteningOnly[,10]~
                                      datLogistic.EasingTighteningOnly[,1],family=binomial(link=logit))
summary(LogisticModel.TighteningEasing_3M)
```

> The p-value of t-test for 3M as predictor is very small, indicating that it is a significant predictor for easing or tightening.

```{r}
matplot(datLogistic.EasingTighteningOnly[,-c(9,10)],type="l",ylab="Data and Fitted Values")
lines(datLogistic.EasingTighteningOnly[,10]*20,col="red")
lines(LogisticModel.TighteningEasing_3M$fitted.values*20,col="green")
```

Fit logistic model with all predictors. Interpret the summary/coefficients, contrast to previous model, and display the plot which overlays the predicted values. Say anything that occurs to you. Consider the *goal*: have we achieved it? Have we achieved it well?

```{r}
#Fit model with all predictors
LogisticModel.TighteningEasing_All<-glm(datLogistic.EasingTighteningOnly[,10]~
                                            datLogistic.EasingTighteningOnly[,1] +
                                            datLogistic.EasingTighteningOnly[,2] +
                                            datLogistic.EasingTighteningOnly[,3] +
                                            datLogistic.EasingTighteningOnly[,4] +
                                            datLogistic.EasingTighteningOnly[,5] +
                                            datLogistic.EasingTighteningOnly[,6] +
                                            datLogistic.EasingTighteningOnly[,7], family=binomial(link=logit))
summary(LogisticModel.TighteningEasing_All)$aic
```

```{r}
summary(LogisticModel.TighteningEasing_All)$coefficients[,c(1,4)]
```

> The only variable that does not appear significant is variable 6.

```{r}
matplot(datLogistic.EasingTighteningOnly[,-c(9,10)],type="l",ylab="Results of Logistic Regression")
lines(datLogistic.EasingTighteningOnly[,10]*20,col="red")
lines(LogisticModel.TighteningEasing_All$fitted.values*20,col="green")
```

> From the plot above we see a much greater variance of the fitted value in the green line compared with the model using just the 3M yield. It might indicate overfitting given when using more predictor variables.

```{r}
# Calculate odds
Log.Odds<-predict(LogisticModel.TighteningEasing_All)
plot(Log.Odds,type="l")
```

> From the two plots above, we see that the plot for the log odds and the fitted values are very similar, just on a different scale. This is because the fitted values represent the probability of easing in at a particular time.

```{r}
Probabilities<-1/(exp(-Log.Odds)+1)
plot(LogisticModel.TighteningEasing_All$fitted.values,type="l",ylab="Fitted Values & Log-Odds")
lines(Probabilities,col="red")
```

> The red line for probability overlays the black line for the fitted values perfectly. The fitted values of the logistic model equal to the probabilities of each observation.


## Step 5
Compare linear regression models with different combinations of predictors.
Select the best combination.
Below we show only two of possible combinations: full model containing all 7 predictors and Null model containing only intercept, but none of the 7 predictors.
Estimate other possible combinations.

```{r}
datRegressionComparison<-data.matrix(dat[,-c(9,10)],rownames.force="automatic")
datRegressionComparison<-dat[,-c(9,10)]
```

```{r}
#Fit Full Model with all 7 predictors and Null without any predictors
RegressionModelComparison.Full<-lm(Output1~., data=datRegressionComparison)
summary(RegressionModelComparison.Full)$coeff
```

```{r}
cbind(R2=summary(RegressionModelComparison.Full)$r.squared, Adjusted.R2=summary(RegressionModelComparison.Full)$adj.r.squared)
```

```{r}
summary(RegressionModelComparison.Full)$df
```

#### Interpret the fitted model. How good is the fit? How significant are the parameters?
>  The full model is perfect with an R-squared of 1 and a virtually 0 residual standard error. All parameters are significant with p-values equal to 0.

```{r}
#NUll model exploration
RegressionModelComparison.Null<-lm(Output1~1,data=datRegressionComparison)
summary(RegressionModelComparison.Null)$coeff
```

```{r}
cbind(R2=summary(RegressionModelComparison.Null)$r.squared, Adjusted.R2=summary(RegressionModelComparison.Null)$adj.r.squared)
```

```{r}
summary(RegressionModelComparison.Null)$df
```

#### Why summary(RegressionModelComparison.Null) does not show R2?
>  By definition R2 is the ratio of the effect variance over the total variance. In this case we do not have effect variance since there are no predictors associated with this model.

Compare models pairwise using anova()
```{r}
anova(RegressionModelComparison.Full,RegressionModelComparison.Null)
```

#### Interpret the results of anova().
>  Based on the ANOVA results, the large F value and the extremely small p-value for the F test shows significant difference between the null model and the full model. The null hypothesis that all betas equal to zero is rejected. In other words, at least one of the beta parameters is different than 0. By looking at the Sum Sq, we can further tell that the larger model is a better fit.

### Repeat the analysis for different combinations of input variables and select the one you think is the best.


```{r}
step(RegressionModelComparison.Full)
```

>  The warning message says that "attempting model selection on an essentially perfect fit is nonsense", which means the step() function doesn't work on a perfect fit. Thus we can try drop1() or add() manually to find a simpler model.

```{r}
#DROP Method
bestmodel.pre<-lm(Output1~.,data=datRegressionComparison)
(myScope<-names(datRegressionComparison)[-which(names(datRegressionComparison)=="Output1")])
```

```{r}
drop1(bestmodel.pre,scope=myScope)
```

> Since the AIC for USGG3YR is the lowest, we should try removing it first.

```{r}
RegressionModelComparison.Reduced1 <- lm(Output1~.,data=datRegressionComparison[,-4])
summary(RegressionModelComparison.Reduced1)
```

> We still have a good fit, so we can proceed with dropping a second term.

```{r}
drop1(RegressionModelComparison.Reduced1)
``` 
 
> USGG10YR has the lowest AIC. Remove USGG10YR.

```{r}
RegressionModelComparison.Reduced2 <- lm(Output1~.,data=datRegressionComparison[,-c(4,6)])
summary(RegressionModelComparison.Reduced2)
```

>The model is still a good fit, try removing another predictor.

```{r}
drop1(RegressionModelComparison.Reduced2)
``` 

> Try removing USGG6M.

```{r}
RegressionModelComparison.Reduced3 <- lm(Output1~.,data=datRegressionComparison[,-c(2,4,6)])
summary(RegressionModelComparison.Reduced3)
```
> Remove additional term.

```{r}
drop1(RegressionModelComparison.Reduced3)
``` 
 
```{r}
RegressionModelComparison.Reduced4 <- lm(Output1~.,data=datRegressionComparison[,-c(2,4,5,6)])
summary(RegressionModelComparison.Reduced4)
``` 

> After removing 4 predictors, we start to see a very slight drop in R2 and Adj-R2.But they are still very close to 1 and we can attempt to drop an additional predictor.

```{r}
drop1(RegressionModelComparison.Reduced4)
``` 
 
> Remove USGG3M.
 
```{r}
RegressionModelComparison.Reduced5 <- lm(Output1~.,data=datRegressionComparison[,-c(1,2,4,5,6)])
summary(RegressionModelComparison.Reduced5)
``` 
 
```{r}
drop1(RegressionModelComparison.Reduced5)
``` 
 
 > Remove USGG30YR.
 
```{r}
RegressionModelComparison.Reduced6 <- lm(Output1~.,data=datRegressionComparison[,-c(1,2,4,5,6,7)])
summary(RegressionModelComparison.Reduced6)
``` 

> At the end we see that the model is still almost perfect with only one predictor -- USGG2YR. The R2 and Adj-R2 for the model are both 0.9966. 

```{r}
(sapply(1:7, function(x) summary(lm(datRegressionComparison$Output1~datRegressionComparison[,x]))$r.squared))
```

>  Summary of Model Selection: 
By checking the R-squared values of all single predictor models we can see that even though there are minor variations, all single predictor models provide a very satisfactory fit, with three of which achieving above 0.99 and the fourth (USGG3YR) the highest. We can still conclude that the best model is the model with only “USGG3YR” as single predictor.

## Step 6
Perform rolling window analysis of the yields data.
```{r}
# Set the window width and window shift parameters for rolling window.
Window.width<-20; Window.shift<-5
```
Run rolling mean values usingrollapply().
```{r}
library(zoo)
```
Calculate rolling mean values for each variable.
```{r}
# Means
all.means<-rollapply(datRegressionComparison,width=Window.width,by=Window.shift,by.column=TRUE, mean)
head(all.means,10)
```

```{r}
# Create points at which rolling means are calculated
Count<-1:length(datRegressionComparison[,1])
Rolling.window.matrix<-rollapply(Count,width=Window.width,by=Window.shift,by.column=FALSE,
          FUN=function(z) z)
Rolling.window.matrix[1:10,]
```

```{r}
# Take middle of each window
Points.of.calculation<-Rolling.window.matrix[,10]
Points.of.calculation[1:10]
```

```{r}
length(Points.of.calculation)
```

```{r}
# Incert means into the total length vector to plot the rolling mean with the original data
Means.forPlot<-rep(NA,length(datRegressionComparison[,1]))
Means.forPlot[Points.of.calculation]<-all.means[,1]
Means.forPlot[1:50]
```

```{r}
# Assemble the matrix to plot the rolling means
cbind(datRegressionComparison[,1],Means.forPlot)[1:50,]
```

```{r}
plot(Means.forPlot,col="red")
lines(datRegressionComparison[,1])
```

```{r}
#Run rolling daily difference standard deviation of each variable
DailyDifference<-diff(as.matrix(datRegressionComparison))
head(DailyDifference)
```

```{r}
rolling.sd<-rollapply(DailyDifference,width=Window.width,by=Window.shift,by.column=TRUE, sd)
head(rolling.sd)
```

```{r}
rolling.dates<-rollapply(datRegressionComparison[-1,],width=Window.width,by=Window.shift,
                         by.column=FALSE,FUN=function(z) rownames(z))
head(rolling.dates)
```

```{r}
rownames(rolling.sd)<-rolling.dates[,10]
head(rolling.sd)
```

```{r}
matplot(rolling.sd[,c(1,5,7,8)],xaxt="n",type="l",col=c("black","red","blue","green"))
axis(side=1,at=1:1656,rownames(rolling.sd))
```

#### Show periods of high volatility. How is volatility related to the level of rates?
>  On one hand, volatility seems proportional to the level of rates. The ranking of volatility is as follows: green (USGG30YR), black(USGG3M), red(USGG5YR) and blue(  USGG10YR). The sequence is not the same as the length of the YTM rates. On the other hand, there are some peaks of volitity at some time for all levels of rates, which are accompanied by the most significant change in rates as it can be expected.

```{r}
# Show periods of high volatility
high.volatility.periods<-rownames(rolling.sd)[rolling.sd[,8]>.5]
high.volatility.periods
```

Fit linear model to rolling window data using 3 months, 5 years and 30 years variables as predictors.
```{r}
# Rolling lm coefficients
Coefficients<-rollapply(datRegressionComparison,width=Window.width,by=Window.shift,by.column=FALSE,
         FUN=function(z) coef(lm(Output1~USGG3M+USGG5YR+USGG30YR,data=as.data.frame(z))))
rolling.dates<-rollapply(datRegressionComparison[,1:8],width=Window.width,by=Window.shift,by.column=FALSE,
                         FUN=function(z) rownames(z))

rownames(Coefficients)<-rolling.dates[,10]
Coefficients[1:10,]

```

Look at pairwise X-Y plots of regression coefficients for the 3M, 5Yr and 30Yr yields as inputs.
```{r}
# Pairs plot of Coefficients
pairs(Coefficients)
```

#### Interpret the pairs plot.
>  The pairs plot shows a negative correlation between the 5Yr and the 30Yr YTM. The other pairs don't display much correlation.

```{r}
# Plot of coefficients
matplot(Coefficients[,-1],xaxt="n",type="l",col=c("black","red","green"))
axis(side=1,at=1:1657,rownames(Coefficients))
```

```{r}
high.slopespread.periods<-rownames(Coefficients)[Coefficients[,3]-Coefficients[,4]>3]
jump.slopes<-rownames(Coefficients)[Coefficients[,3]>3]
high.slopespread.periods
```

```{r}
jump.slopes
```

#### Is the picture of coefficients consistent with the picture of pairs? If yes, explain why.
>  Yes, the picture of coefficients is consistent with the pairs picture. The red line goes in opposite direction against black and green, which indicates the corresponding negative correlation between 5Yr and 30Yr YTM. There is a cycling pattern of merging and departing between the red and green lines before 2008, while the coefficients of three lines remain merging together in the post 2008 era.

How often the R-squared is not considered high?
```{r}
# R-squared
r.squared<-rollapply(datRegressionComparison,width=Window.width,by=Window.shift,by.column=FALSE,
         FUN=function(z) summary(lm(Output1~USGG3M+USGG5YR+USGG30YR,data=as.data.frame(z)))$r.squared)
r.squared<-cbind(rolling.dates[,10],r.squared)
r.squared[1:10,]
```

```{r}
plot(r.squared[,2],xaxt="n",ylim=c(0,1))
axis(side=1,at=1:1657,rownames(Coefficients))
```

```{r}
(low.r.squared.periods<-r.squared[r.squared[,2]<.9,1])
```

#### What could cause decrease of R2?
>  As shown above, there are 4 rolling periods with comparatively low R2. A decrease of R2 indicates that the model loses the capacity to predict the observed data. The cause might be some outliers -- some event driven unusual yields in the market.

Analyze the rolling p-values.
```{r}
# P-values
Pvalues<-rollapply(datRegressionComparison,width=Window.width,by=Window.shift,by.column=FALSE,
                        FUN=function(z) summary(lm(Output1~USGG3M+USGG5YR+USGG30YR,data=as.data.frame(z)))$coefficients[,4])
rownames(Pvalues)<-rolling.dates[,10]
Pvalues[1:10,]
```

```{r}
matplot(Pvalues,xaxt="n",col=c("black","blue","red","green"),type="o")
axis(side=1,at=1:1657,rownames(Coefficients))
```

```{r}
rownames(Pvalues)[Pvalues[,2]>.5]
```

```{r}
rownames(Pvalues)[Pvalues[,3]>.5]
```

```{r}
rownames(Pvalues)[Pvalues[,4]>.5]
```

#### Interpret the plot.
>  The plot shows that the USGG30YR has the most frequent occurrence weak significance of p-value (as shown in green), and the USGG5YR has the least occurrence of high p-value (as shown in red). This is consistent with the length of the lists with high p-value we printed above. Since the plot of p-value help identify if one set of the predictors is more or less important than the others through various periods, we can conclude that the USGG3M and especially the USGG5YR are the most stable predictor through the whole obersed period. However, for periods in 12/1982, 1987, 9/1988, 12/1999,  USGG5YR is not reliable while USGG3M works as a better predictor. 

## Step 7
Perform PCA with the inputs (columns 1-7).
```{r}
dat.Output<-dat$Output1
dat<-data.matrix(dat[,1:7],rownames.force="automatic")
dim(dat)
```

```{r}
head(dat)
```

```{r}
# Select 3 variables. Explore dimensionality and correlation 
dat.3M_2Y_5Y<-dat[,c(1,3,5)]
pairs(dat.3M_2Y_5Y)
```

Observe the 3D plot of the set. Use library rgl:
```{r,warning=FALSE}
#library(rgl)
#rgl.points(dat.3M_2Y_5Y)
#rgl seems to have problems with the OSX. Using plotly instead.
library(plotly)
p <- plot_ly(data.frame(dat.3M_2Y_5Y), x = ~USGG3M, y = ~USGG2YR, z = ~USGG5YR) %>%
# add_markers() %>%
layout(scene = list(xaxis = list(title = 'USGG3M'),
                     yaxis = list(title = 'USGG2YR'),
                     zaxis = list(title = 'USGG5YR')))
p
```

```{r}
#Manual.Covariance.Matrix 
D_Assgndata<- apply(dat, 2, function(x) x-mean(x))
Manual.Covariance.Matrix<-(nrow(dat)-1)^-1 * t(D_Assgndata) %*% D_Assgndata
Manual.Covariance.Matrix
```

```{r}
Covariance.Matrix<-cov(dat)
Covariance.Matrix
```

```{r}
# Plot the covariance matrix.
Maturities<-c(.25,.5,2,3,5,10,30)
contour(Maturities,Maturities,Covariance.Matrix)
```

```{r}
#Manually calculate PCA
#Zero Loading
Means<-apply(dat, 2, function(x) mean(x))
testdata<-apply(dat,2,function(x) x-mean(x))
Eigen.Decomposition<-eigen(cov(dat))
Factors<-testdata %*% Eigen.Decomposition$vectors
#The first 3 Loadings
head(Factors[,1:3])
```

```{r}
barplot(Eigen.Decomposition$values/sum(Eigen.Decomposition$values),width=2,col="black",names.arg = c("F1","F2","F3","F4","F5","F6","F7"))
```

```{r}
#The first 3 loadings
Loadings<-Eigen.Decomposition$vectors[,1:3]
matplot(Maturities,Loadings,type="l",lty=1,col=c("black","red","green"),lwd=3)
```

#### Interpret the factors by looking at the shapes of the loadings.
>  
Factor 1, colored black, is a fairly flat line below 0, which means it will produce a downward shift in all the yields for all maturities.
Factor 2, colored red, will cause a twist or tilt effect by causing a downward shift on the short term maturity yields and an upwards shift on long term maturity yields.
Factor 3, colored green, will cause a butterfly effect by bringing a decrease on the mid term maturities and an upwards shift on both short-term and long-term maturities.

```{r}
# Calculate and plot 3 selected factors
matplot(Factors,type="l",col=c("black","red","green"),lty=1,lwd=3)
```

```{r}
# Change the signs of the first factor and the corresponding factor loading.
Loadings[,1]<--Loadings[,1]
Factors[,1]<--Factors[,1]
matplot(Factors,type="l",col=c("black","red","green"),lty=1,lwd=3)
```

```{r}
matplot(Maturities,Loadings,type="l",lty=1,col=c("black","red","green"),lwd=3)
```

```{r}
plot(Factors[,1],Factors[,2],type="l",lwd=2)
```

#### Draw at least three conclusions from the plot of the first two factors above.
> 
Conclusion 1: In theory, Factor 1 and Factor 2 should be uncorrelated given their orthogonal nature, and it is true if we look at the whole time-period. However, in certain periods in our data, they might be highly correlated, either positive or negative.
Conclusion 2: The plot is a time-series plot reflecting the interaction between Factor 1 and Factor 2 and has economic rational behind it. On the right hand of the plot, we see high volatility between the two factors as the scatterplot is way more spread out. (data point from 1 to 1000, or 1981 through early 1985 period, marked in orange dots shown below)
Conclusion 3: As we can see from the second graph below, the blue, red and green points corresponding to period 1996-2001,2004-2006,2007-2014 show patterns of positive correlation, negative correlation and clockwise movement between Factor 1 and Factor 2 respectively. So basically, all movements of the two factors can be categorized as “coupling”, “decoupling” or “lagging”.

```{r}
# Periods under consideration 
rownames(dat)[c(1,1000,3800,5100,5900,6500,7000,8300)]
```

```{r}
plot(Factors[,1],Factors[,2],type="l",lwd=2)
points(Factors[1:800,1],Factors[1:800,2], col="orange")
points(Factors[7000:8300,1],Factors[7000:8300,2], col="blue")
points(Factors[5900:6500,1],Factors[5900:6500,2], col="red")
points(Factors[3800:5100,1],Factors[3800:5100,2], col="green")
```

Analyze the adjustments that each factor makes to the term curve.
```{r}
OldCurve<-dat[135,]
NewCurve<-dat[136,]
CurveChange<-NewCurve-OldCurve
FactorsChange<-Factors[136,]-Factors[135,]
ModelCurveAdjustment.1Factor<-OldCurve+t(Loadings[,1])*FactorsChange[1]
ModelCurveAdjustment.2Factors<-OldCurve+t(Loadings[,1])*FactorsChange[1]+t(Loadings[,2])*FactorsChange[2]
ModelCurveAdjustment.3Factors<-OldCurve+t(Loadings[,1])*FactorsChange[1]+t(Loadings[,2])*FactorsChange[2]+
  t(Loadings[,3])*FactorsChange[3]
matplot(Maturities,
        t(rbind(OldCurve,NewCurve,ModelCurveAdjustment.1Factor,ModelCurveAdjustment.2Factors,
                ModelCurveAdjustment.3Factors)),
        type="l",lty=c(1,1,2,2,2),col=c("black","red","green","blue","magenta"),lwd=3,ylab="Curve Adjustment")
legend(x="topright",c("Old Curve","New Curve","1-Factor Adj.","2-Factor Adj.",
                      "3-Factor Adj."),lty=c(1,1,2,2,2),lwd=3,col=c("black","red","green","blue","magenta"))
```

```{r}
rbind(CurveChange,ModelCurveAdjustment.3Factors-OldCurve)
```

#### Explain how shapes of the loadings affect the adjustnents using only factor 1, factors 1 and 2, and all 3 factors.
>  Factor 1 (the green line) lifts the old line up by a uniform level. Factor 2 (the blue line) is causing the twisting or clockwise rotating effect -- the part before the peak is lifted up and the part after the peak is drawn lower. Factor 3 (the pink line) brings in the butterfly effect by affecting the central portion and the extremes in oppposite ways.

See the goodness of fit for the example of 10Y yield.
```{r}
# How close is the approximation for each maturity?
# 5Y
cbind(Maturities,Loadings)
```

```{r}
Model.10Y<-Means[6]+Loadings[6,1]*Factors[,1]+Loadings[6,2]*Factors[,2]+Loadings[6,3]*Factors[,3]
matplot(cbind(dat[,6],Model.10Y),type="l",lty=1,lwd=c(3,1),col=c("black","red"),ylab="5Y Yield")
```

```{r}
# Repeat the PCA using princomp.
# Do PCA analysis using princomp()
PCA.Yields<-princomp(dat)
names(PCA.Yields)
```

```{r}
# Check that the loadings are the same
cbind(PCA.Yields$loadings[,1:3],Maturities,Eigen.Decomposition$vectors[,1:3])
```

```{r}
matplot(Maturities,PCA.Yields$loadings[,1:3],type="l",col=c("black","red","green"),lty=1,lwd=3)
```

```{r}
matplot(PCA.Yields$scores[,1:3],type="l",col=c("black","red","green"),lwd=3,lty=1)
```

```{r}
# Change the signs of the 1st factor and the first loading
PCA.Yields$loadings[,1]<--PCA.Yields$loadings[,1]
PCA.Yields$scores[,1]<--PCA.Yields$scores[,1]
matplot(Maturities,PCA.Yields$loadings[,1:3],type="l",col=c("black","red","green"),lty=1,lwd=3)
```

```{r}
matplot(PCA.Yields$scores[,1:3],type="l",col=c("black","red","green"),lwd=3,lty=1)
```

Uncover the mystery of the Output in column 8.
```{r}
# What variable we had as Output?
matplot(cbind(PCA.Yields$scores[,1],dat.Output,Factors[,1]),type="l",col=c("black","red","green"),lwd=c(3,2,1),lty=c(1,2,3),ylab="Factor 1")
```

Compare the regression coefficients from Step 2 and Step 3 with factor loadings.
First, look at the slopes for dat.Input~dat.Output
```{r}
t(apply(dat, 2, function(dat.col) lm(dat.col~dat.Output)$coef))
```

```{r}
cbind(PCA.Yields$center,PCA.Yields$loadings[,1])
```

This shows that the zero loading equals the vector of intercepts of models Y~Output1, where Y is one of the columns of yields in the data. Also, the slopes of the same models are equal to the first loading.

Check if the same is true in the opposite direction: is there a correspondence between the coefficients of models Output1~Yield and the first loading.

```{r}
dat.Centered<-t(apply(dat,1,function(dat.row) dat.row-PCA.Yields$center))
dim(dat.Centered)
```

```{r}
t(apply(dat.Centered, 2, function(dat.col) lm(dat.Output~dat.col)$coef))
```

To recover the loading of the first factor by doing regression, use all inputs together.
```{r}
t(lm(dat.Output~dat.Centered)$coef)[-1]
```

```{r}
PCA.Yields$loadings[,1]
```
This means that the factor is a portfolio of all input variables with weights.
